{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3urfvvm165iantk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version                                               data\n",
       "0        1  {'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...\n",
       "1        1  {'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...\n",
       "2        1  {'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...\n",
       "3        1  {'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...\n",
       "4        1  {'source': 'gutenberg', 'id': '3urfvvm165iantk..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CoQA is a Conversational Question Answering dataset released by Stanford NLP in 2019. It is a large-scale dataset for building Conversational Question Answering Systems. \n",
    "coqa = pd.read_json('http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json')\n",
    "coqa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning. Get rid of version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del coqa[\"version\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'source': 'gutenberg', 'id': '3urfvvm165iantk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data\n",
       "0  {'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...\n",
       "1  {'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...\n",
       "2  {'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...\n",
       "3  {'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...\n",
       "4  {'source': 'gutenberg', 'id': '3urfvvm165iantk..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coqa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every question-answer pair, we will be attaching the linked story to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required columns in our dataframe\n",
    "cols = [\"text\",\"question\",\"answer\"]\n",
    "#list of lists to create our dataframe\n",
    "comp_list = []\n",
    "for index, row in coqa.iterrows():\n",
    "    for i in range(len(row[\"data\"][\"questions\"])):\n",
    "        temp_list = []\n",
    "        temp_list.append(row[\"data\"][\"story\"])\n",
    "        temp_list.append(row[\"data\"][\"questions\"][i][\"input_text\"])\n",
    "        temp_list.append(row[\"data\"][\"answers\"][i][\"input_text\"])\n",
    "        comp_list.append(temp_list)\n",
    "new_df = pd.DataFrame(comp_list, columns=cols) \n",
    "#saving the dataframe to csv file for further loading\n",
    "new_df.to_csv(\"CoQA_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading from Local CSV file which is the cleaned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108642</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>Who was a sub?</td>\n",
       "      <td>Xabi Alonso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108643</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>Was it his first game this year?</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108644</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>What position did the team reach?</td>\n",
       "      <td>third</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108645</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>Who was ahead of them?</td>\n",
       "      <td>Barca.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108646</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>By how much?</td>\n",
       "      <td>six points</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "108642  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "108643  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "108644  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "108645  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "108646  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "\n",
       "                                 question       answer  \n",
       "108642                     Who was a sub?  Xabi Alonso  \n",
       "108643   Was it his first game this year?          Yes  \n",
       "108644  What position did the team reach?        third  \n",
       "108645             Who was ahead of them?       Barca.  \n",
       "108646                       By how much?   six points  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"CoQA_data.csv\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some profile info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question and answers:  108647\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of question and answers: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking pretrained models for BertForQuestionAnswering class from the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', return_dict=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random testing of question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 278 tokens.\n",
      "[CLS]        101\n",
      "what       2,054\n",
      "was        2,001\n",
      "it         2,009\n",
      "considered   2,641\n",
      "?          1,029\n",
      "[SEP]        102\n",
      "germany    2,762\n",
      "is         2,003\n",
      "a          1,037\n",
      "federal    2,976\n",
      "republic   3,072\n",
      "consisting   5,398\n",
      "of         1,997\n",
      "sixteen    7,032\n",
      "federal    2,976\n",
      "states     2,163\n",
      "(          1,006\n",
      "german     2,446\n",
      ":          1,024\n",
      "bun       21,122\n",
      "##des      6,155\n",
      "##land     3,122\n",
      ",          1,010\n",
      "or         2,030\n",
      "land       2,455\n",
      ")          1,007\n",
      ".          1,012\n",
      "[          1,031\n",
      "a          1,037\n",
      "]          1,033\n",
      "since      2,144\n",
      "today      2,651\n",
      "'          1,005\n",
      "s          1,055\n",
      "germany    2,762\n",
      "was        2,001\n",
      "formed     2,719\n",
      "from       2,013\n",
      "an         2,019\n",
      "earlier    3,041\n",
      "collection   3,074\n",
      "of         1,997\n",
      "several    2,195\n",
      "states     2,163\n",
      ",          1,010\n",
      "it         2,009\n",
      "has        2,038\n",
      "a          1,037\n",
      "federal    2,976\n",
      "constitution   4,552\n",
      ",          1,010\n",
      "and        1,998\n",
      "the        1,996\n",
      "constituent  13,794\n",
      "states     2,163\n",
      "retain     9,279\n",
      "a          1,037\n",
      "measure    5,468\n",
      "of         1,997\n",
      "sovereignty  12,601\n",
      ".          1,012\n",
      "with       2,007\n",
      "an         2,019\n",
      "emphasis   7,902\n",
      "on         2,006\n",
      "geographical  10,056\n",
      "conditions   3,785\n",
      ",          1,010\n",
      "berlin     4,068\n",
      "and        1,998\n",
      "hamburg    8,719\n",
      "are        2,024\n",
      "frequently   4,703\n",
      "called     2,170\n",
      "st         2,358\n",
      "##adt     18,727\n",
      "##sta      9,153\n",
      "##ate      3,686\n",
      "##n        2,078\n",
      "(          1,006\n",
      "city       2,103\n",
      "-          1,011\n",
      "states     2,163\n",
      ")          1,007\n",
      ",          1,010\n",
      "as         2,004\n",
      "is         2,003\n",
      "the        1,996\n",
      "free       2,489\n",
      "hans       7,003\n",
      "##ea       5,243\n",
      "##tic      4,588\n",
      "city       2,103\n",
      "of         1,997\n",
      "bremen    16,314\n",
      ",          1,010\n",
      "which      2,029\n",
      "in         1,999\n",
      "fact       2,755\n",
      "includes   2,950\n",
      "the        1,996\n",
      "cities     3,655\n",
      "of         1,997\n",
      "bremen    16,314\n",
      "and        1,998\n",
      "br         7,987\n",
      "##eme     21,382\n",
      "##rh      25,032\n",
      "##ave     10,696\n",
      "##n        2,078\n",
      ".          1,012\n",
      "the        1,996\n",
      "remaining   3,588\n",
      "13         2,410\n",
      "states     2,163\n",
      "are        2,024\n",
      "called     2,170\n",
      "fl        13,109\n",
      "##ache    15,395\n",
      "##nl      20,554\n",
      "##ander   12,243\n",
      "(          1,006\n",
      "literally   6,719\n",
      ":          1,024\n",
      "area       2,181\n",
      "states     2,163\n",
      ")          1,007\n",
      ".          1,012\n",
      "the        1,996\n",
      "creation   4,325\n",
      "of         1,997\n",
      "the        1,996\n",
      "federal    2,976\n",
      "republic   3,072\n",
      "of         1,997\n",
      "germany    2,762\n",
      "in         1,999\n",
      "1949       4,085\n",
      "was        2,001\n",
      "through    2,083\n",
      "the        1,996\n",
      "unification  16,905\n",
      "of         1,997\n",
      "the        1,996\n",
      "western    2,530\n",
      "states     2,163\n",
      "(          1,006\n",
      "which      2,029\n",
      "were       2,020\n",
      "previously   3,130\n",
      "under      2,104\n",
      "american   2,137\n",
      ",          1,010\n",
      "british    2,329\n",
      ",          1,010\n",
      "and        1,998\n",
      "french     2,413\n",
      "administration   3,447\n",
      ")          1,007\n",
      "created    2,580\n",
      "in         1,999\n",
      "the        1,996\n",
      "aftermath  10,530\n",
      "of         1,997\n",
      "world      2,088\n",
      "war        2,162\n",
      "ii         2,462\n",
      ".          1,012\n",
      "initially   3,322\n",
      ",          1,010\n",
      "in         1,999\n",
      "1949       4,085\n",
      ",          1,010\n",
      "the        1,996\n",
      "states     2,163\n",
      "of         1,997\n",
      "the        1,996\n",
      "federal    2,976\n",
      "republic   3,072\n",
      "were       2,020\n",
      "baden     12,189\n",
      ",          1,010\n",
      "bavaria   11,606\n",
      "(          1,006\n",
      "in         1,999\n",
      "german     2,446\n",
      ":          1,024\n",
      "bayern    21,350\n",
      ")          1,007\n",
      ",          1,010\n",
      "bremen    16,314\n",
      ",          1,010\n",
      "hamburg    8,719\n",
      ",          1,010\n",
      "hesse     16,399\n",
      "(          1,006\n",
      "hesse     16,399\n",
      "##n        2,078\n",
      ")          1,007\n",
      ",          1,010\n",
      "lower      2,896\n",
      "saxony    13,019\n",
      "(          1,006\n",
      "ni         9,152\n",
      "##ede     14,728\n",
      "##rsa     22,381\n",
      "##chs     18,069\n",
      "##en       2,368\n",
      ")          1,007\n",
      ",          1,010\n",
      "north      2,167\n",
      "rhine     10,950\n",
      "westphalia  22,952\n",
      "(          1,006\n",
      "nord      13,926\n",
      "##rh      25,032\n",
      "##ein     12,377\n",
      "-          1,011\n",
      "west       2,225\n",
      "##fa       7,011\n",
      "##len      7,770\n",
      ")          1,007\n",
      ",          1,010\n",
      "rhineland  21,640\n",
      "-          1,011\n",
      "palatinate  18,990\n",
      "(          1,006\n",
      "r          1,054\n",
      "##hein    26,496\n",
      "##land     3,122\n",
      "-          1,011\n",
      "p          1,052\n",
      "##fa       7,011\n",
      "##lz      23,858\n",
      ")          1,007\n",
      ",          1,010\n",
      "schleswig  21,173\n",
      "-          1,011\n",
      "holstein  18,864\n",
      ",          1,010\n",
      "wurttemberg  16,346\n",
      "-          1,011\n",
      "baden     12,189\n",
      ",          1,010\n",
      "and        1,998\n",
      "wurttemberg  16,346\n",
      "-          1,011\n",
      "ho         7,570\n",
      "##hen     10,222\n",
      "##zo       6,844\n",
      "##ller    10,820\n",
      "##n        2,078\n",
      ".          1,012\n",
      "west       2,225\n",
      "berlin     4,068\n",
      ",          1,010\n",
      "while      2,096\n",
      "officially   3,985\n",
      "not        2,025\n",
      "part       2,112\n",
      "of         1,997\n",
      "the        1,996\n",
      "federal    2,976\n",
      "republic   3,072\n",
      ",          1,010\n",
      "was        2,001\n",
      "largely    4,321\n",
      "integrated   6,377\n",
      "and        1,998\n",
      "considered   2,641\n",
      "as         2,004\n",
      "a          1,037\n",
      "de         2,139\n",
      "facto     13,743\n",
      "state      2,110\n",
      ".          1,012\n",
      "[SEP]        102\n"
     ]
    }
   ],
   "source": [
    "# Random Qn and test pair\n",
    "random_num = np.random.randint(0,len(data))\n",
    "question = data[\"question\"][random_num]\n",
    "text = data[\"text\"][random_num]\n",
    "\n",
    "# qiestopm amd text pair testing. Let‚Äôs tokenize the question and text as a pair.\n",
    "input_ids = tokenizer.encode(question, text)\n",
    "#print(\"here is how the input_ids look like\", input_ids)\n",
    "print(\"The input has a total of {} tokens.\".format(len(input_ids)))\n",
    "\n",
    "# to see what the tokernizer is doing.\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    print('{:8}{:8,}'.format(token,id))\n",
    "\n",
    "# for [CLS] vs [SEP] see https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626#:~:text=we%20can%20see%20two%20special%20tokens%20%5Bcls%5D%20and%20%5Bsep%5D.%20\n",
    "\n",
    "# [CLS] token stands for classification and is there to represent sentence-level classification and is used when we are classifying. Another token used by BERT is [SEP]. It is used to separate the two pieces of text. You can see two [SEP] tokens in the above screenshots, one after the question and another after the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers library can create segment embeddings on its own using PretrainedTokenizer.encode_plus(). But, we can even create our own. For that, we just need to specify a 0 or 1 for each token.  Segment embeddings help BERT in differentiating a question from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP token index:  6\n",
      "Number of tokens in segment A:  7\n",
      "Number of tokens in segment B:  271\n",
      "segment_IDs [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Number of segment_ids 278\n"
     ]
    }
   ],
   "source": [
    "#first occurence of [SEP] token\n",
    "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "print(\"SEP token index: \", sep_idx)\n",
    "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
    "num_seg_a = sep_idx+1\n",
    "print(\"Number of tokens in segment A: \", num_seg_a)\n",
    "#number of tokens in segment B (text)\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "print(\"Number of tokens in segment B: \", num_seg_b)\n",
    "#creating the segment ids\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "print(\"segment_IDs\", segment_ids)\n",
    "print(\"Number of segment_ids\", len(segment_ids))\n",
    "#making sure that every input token has a segment id\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
    "output = model(torch.tensor([input_ids]), \n",
    "token_type_ids=torch.tensor([segment_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most probable start and end words and providing answers only if the end token is after the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "What was it considered?\n",
      "\n",
      "Answer:\n",
      "De facto state.\n"
     ]
    }
   ],
   "source": [
    "#tokens with highest start and end scores\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "    \n",
    "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
    "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Question:\n",
    "Who is the acas director?\n",
    "\n",
    "Answer:\n",
    "Agnes karin ##gu. -->\n",
    "\n",
    "<!-- what‚Äôs this ‚Äú##‚Äù in the reply? Keep on reading! üìô\n",
    "BERT uses wordpiece tokenization. In BERT, rare words get broken down into subwords/pieces. Wordpiece tokenization uses ## to delimit tokens that have been split. An example of this: ‚ÄúKarin‚Äù is a common word so wordpiece does not split it. However, ‚ÄúKaringu‚Äù is a rare word so wordpiece split it into the words, ‚ÄúKarin‚Äù and ‚Äú##gu‚Äù. Notice that it has added ## before gu to indicate that it is the second piece of the split word.\n",
    "The idea behind using wordpiece tokenization is to reduce the size of the vocabulary which improves training performance. Consider the words, run, running, runner. Without wordpiece tokenization, the model has to store and learn the meaning of all three words independently. However, with wordpiece tokenization, each of the three words would be split into ‚Äòrun‚Äô and the related ‚Äò##SUFFIX‚Äô (if any suffix at all ‚Äî for example, ‚Äúrun‚Äù, ‚Äú##ning‚Äù, ‚Äú##ner‚Äù). Now, the model will learn the context of the word ‚Äúrun‚Äù and the rest of the meaning would be encoded in the suffix, which would be learned from other words with similar suffixes. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = tokens[answer_start]\n",
    "for i in range(answer_start+1, answer_end+1):\n",
    "    if tokens[i][0:2] == \"##\":\n",
    "        answer += tokens[i][2:]\n",
    "    else:\n",
    "        answer += \" \" + tokens[i]\n",
    "# The above answer will now become: Agnes karingu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA into a function easy for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, text):\n",
    "    \n",
    "    #tokenize question and text as a pair\n",
    "    input_ids = tokenizer.encode(question, text)\n",
    "    \n",
    "    #string version of tokenized ids\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    #segment IDs\n",
    "    #first occurence of [SEP] token\n",
    "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "    #number of tokens in segment A (question)\n",
    "    num_seg_a = sep_idx+1\n",
    "    #number of tokens in segment B (text)\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    \n",
    "    #list of 0s and 1s for segment embeddings\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    \n",
    "    #model output using input_ids and segment_ids\n",
    "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "    \n",
    "    #reconstructing the answer\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits)\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "                \n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    \n",
    "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test QA with NY text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted answer:\n",
      "Hard rock cafe in new york ' s times square\n",
      "Original answer:\n",
      " Hard Rock Cafe\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \\\"Motown 25,\\\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \\\"Clyde\\\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. \"The legacy that [Jackson] left behind is bigger than life for me,\\\" Orange said. \\\"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\\\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.\"\"\"\n",
    "question = \"Where was the Auction held?\"\n",
    "question_answer(question, text)\n",
    "#original answer from the dataset\n",
    "print(\"Original answer:\\n\", data.loc[data[\"question\"] == question][\"answer\"].values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATBOT STYLE QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please enter your text: \n",
    "#The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula.   The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail.   In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online.   The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items.   Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican.   The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.\n",
    "\n",
    "# Q1 When was the Vat formally opened?\n",
    "# Q2 How many books does it have?\n",
    "# Q3 What is the library for?\n",
    "# Q4 What are the periods of the library history\n",
    "# Q5 How many books survive the earliest days of the church\n",
    "# Q6 Is the library online?\n",
    "\n",
    "# wrong answers\n",
    "#Q7 How many giraffes are there\n",
    "\n",
    "# no answers\n",
    "#Q8 when will the collection be available online?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted answer:\n",
      "1475\n",
      "\n",
      "Predicted answer:\n",
      "1 . 1 million\n",
      "\n",
      "Predicted answer:\n",
      "Research library for history , law , philosophy , science and theology\n",
      "\n",
      "Predicted answer:\n",
      "Pre - lateran , lateran , avignon , pre - vatican and vatican\n",
      "\n",
      "Predicted answer:\n",
      "Only a handful of volumes\n",
      "\n",
      "Predicted answer:\n",
      "The vatican library began an initial four - year project of digitising its collection of manuscripts , to be made available online\n",
      "\n",
      "Predicted answer:\n",
      "8 , 500\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Please enter your text: \\n\")\n",
    "question = input(\"\\nPlease enter your question: \\n\")\n",
    "while True:\n",
    "    question_answer(question, text)\n",
    "    \n",
    "    flag = True\n",
    "    flag_N = False\n",
    "    \n",
    "    while flag:\n",
    "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
    "        if response[0] == \"Y\":\n",
    "            question = input(\"\\nPlease enter your question: \\n\")\n",
    "            flag = False\n",
    "        elif response[0] == \"N\":\n",
    "            print(\"\\nBye!\")\n",
    "            flag = False\n",
    "            flag_N = True\n",
    "            \n",
    "    if flag_N == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Short Text Comparison between Bert and Longformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal data refers to data about an individual who can be identified from that data, or from that data and other information to which the organisation has or is likely to have access. \n",
    "\n",
    "The Personal Data Protection Act (PDPA) provides a baseline standard of protection for personal data in Singapore. It complements sector-specific legislative and regulatory frameworks such as the Banking Act and Insurance Act.\n",
    "\n",
    "It comprises various requirements governing the collection, use, disclosure and care of personal data in Singapore. \n",
    "\n",
    "It also provides for the establishment of a national Do Not Call (DNC) Registry. Individuals may register their Singapore telephone numbers with the DNC Registry to opt out of receiving unwanted telemarketing messages from organisations.\n",
    "\n",
    "The PDPA recognises both the need to protect individuals‚Äô personal data and the need of organisations to collect, use or disclose personal data for legitimate and reasonable purposes.\n",
    "\n",
    "A data protection regime is necessary to safeguard personal data from misuse and to maintain individuals‚Äô trust in organisations that manage their data.\n",
    "\n",
    "By regulating the flow of personal data among organisations, the PDPA also aims to strengthen Singapore‚Äôs position as a trusted hub for businesses.\n",
    "\n",
    "The PDPA covers personal data stored in electronic and non-electronic formats. \n",
    "\n",
    "It generally does not apply to any individual acting on a personal or domestic basis, any individual acting in his/her capacity as an employee with an organisation , any public agency in relation to the collection, use or disclosure of personal data, any business contact information such as an individual‚Äôs name, position or title, business telephone number, business address, business email, business fax number and similar information and organisations are required to comply with the various data protection obligations if they undertake activities relating to the collection, use or disclosure of personal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for PDPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Personal Data?\n",
    "# Q2. Does it overrides the other pieces of legistrations?\n",
    "# Q3. What is the purpose of PDPA?\n",
    "# Q4. What do the organisations need to do to ensure they comply with PDPA?/ How do organisations comply with the PDPA? **\n",
    "# Q5. What are not within the scope of PDPA?\n",
    "# Q6. How many giraffes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = pd.read_csv(r'shorttext.txt',header=None,sep='\\t')\n",
    "# word count = 294\n",
    "text = \"Personal data refers to data about an individual who can be identified from that data, or from that data and other information to which the organisation has or is likely to have access. The Personal Data Protection Act (PDPA) provides a baseline standard of protection for personal data in Singapore. It complements sector-specific legislative and regulatory frameworks such as the Banking Act and Insurance Act. It comprises various requirements governing the collection, use, disclosure and care of personal data in Singapore. It also provides for the establishment of a national Do Not Call (DNC) Registry. Individuals may register their Singapore telephone numbers with the DNC Registry to opt out of receiving unwanted telemarketing messages from organisations. The PDPA recognises both the need to protect individuals‚Äô personal data and the need of organisations to collect, use or disclose personal data for legitimate and reasonable purposes. A data protection regime is necessary to safeguard personal data from misuse and to maintain individuals‚Äô trust in organisations that manage their data. By regulating the flow of personal data among organisations, the PDPA also aims to strengthen Singapore‚Äôs position as a trusted hub for businesses. The PDPA covers personal data stored in electronic and non-electronic formats. It generally does not apply to any individual acting on a personal or domestic basis, any individual acting in his/her capacity as an employee with an organisation , any public agency in relation to the collection, use or disclosure of personal data, any business contact information such as an individual‚Äôs name, position or title, business telephone number, business address, business email, business fax number and similar information and organisations are required to comply with the various data protection obligations if they undertake activities relating to the collection, use or disclosure of personal data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal data refers to data about an individual who can be identified from that data, or from that data and other information to which the organisation has or is likely to have access. The Personal Data Protection Act (PDPA) provides a baseline standard of protection for personal data in Singapore. It complements sector-specific legislative and regulatory frameworks such as the Banking Act and Insurance Act. It comprises various requirements governing the collection, use, disclosure and care of personal data in Singapore. It also provides for the establishment of a national Do Not Call (DNC) Registry. Individuals may register their Singapore telephone numbers with the DNC Registry to opt out of receiving unwanted telemarketing messages from organisations. The PDPA recognises both the need to protect individuals‚Äô personal data and the need of organisations to collect, use or disclose personal data for legitimate and reasonable purposes. A data protection regime is necessary to safeguard personal data from misuse and to maintain individuals‚Äô trust in organisations that manage their data. By regulating the flow of personal data among organisations, the PDPA also aims to strengthen Singapore‚Äôs position as a trusted hub for businesses. The PDPA covers personal data stored in electronic and non-electronic formats. It generally does not apply to any individual acting on a personal or domestic basis, any individual acting in his/her capacity as an employee with an organisation , any public agency in relation to the collection, use or disclosure of personal data, any business contact information such as an individual‚Äôs name, position or title, business telephone number, business address, business email, business fax number and similar information and organisations are required to comply with the various data protection obligations if they undertake activities relating to the collection, use or disclosure of personal data.\n"
     ]
    }
   ],
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted answer:\n",
      "Data about an individual\n",
      "\n",
      "Predicted answer:\n",
      "It complements sector - specific legislative and regulatory frameworks such as the banking act and insurance act .\n",
      "\n",
      "Predicted answer:\n",
      "Provides a baseline standard of protection for personal data in singapore\n",
      "\n",
      "Predicted answer:\n",
      "If they undertake activities relating to the collection , use or disclosure of personal data\n",
      "\n",
      "Predicted answer:\n",
      "Any individual acting on a personal or domestic basis , any individual acting in his / her capacity as an employee with an organisation , any public agency in relation to the collection , use or disclosure of personal data , any business contact information\n",
      "\n",
      "Predicted answer:\n",
      "Unable to find the answer to your question.\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# 2 options to input text. Either 1. assign to a variable text or 2. input into the chatbot at start\n",
    "# text = input(\"Please enter your text: \\n\")\n",
    "question = input(\"\\nPlease enter your question: \\n\")\n",
    "while True:\n",
    "    question_answer(question, text)\n",
    "    \n",
    "    flag = True\n",
    "    flag_N = False\n",
    "    \n",
    "    while flag:\n",
    "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
    "        if response[0] == \"Y\":\n",
    "            question = input(\"\\nPlease enter your question: \\n\")\n",
    "            flag = False\n",
    "        elif response[0] == \"N\":\n",
    "            print(\"\\nBye!\")\n",
    "            flag = False\n",
    "            flag_N = True\n",
    "            \n",
    "    if flag_N == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT is not for long text \n",
    "limited at 512 tokens. Performance on long text by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count = 1489\n",
    "text = \"Chow Yun-fat SBS (born 18 May 1955), previously known as Donald Chow, is a Hong Kong actor known for his collaborations with filmmaker John Woo in the action heroic bloodshed films A Better Tomorrow, The Killer, and Hard Boiled, and in the West for his roles as Li Mu-bai in Crouching Tiger, Hidden Dragon and Sao Feng in Pirates of the Caribbean: At World's End. He mainly plays in drama films and has won three Hong Kong Film Awards for Best Actor and two Golden Horse Awards for Best Actor in Taiwan. Chow started his career in movies in 1976 with Goldig Films. Chow was born in Lamma Island, Hong Kong, to Chow Yung-Wan (Âë®ÂÆπÂÖÅ), who worked on a Shell Oil Company tanker, and Chan Lai-fong (Èô≥È∫óËä≥), who was a cleaning lady and vegetable farmer. Chow grew up in a farming community on Lamma Island, in a house with no electricity. He woke up at dawn each morning to help his mother sell herbal jelly and Hakka tea-pudding (ÂÆ¢ÂÆ∂Ëå∂Á≤ø) on the streets; in the afternoons, he went to work in the fields. His family moved to Kowloon when he was ten. At 17, Chow left school to help support the family by doing odd jobs including a bellboy, postman, camera salesman, and taxi driver. Chow's life started to change after college when he responded to a newspaper advertisement, and his actor-trainee application was accepted by TVB, the local television station. He signed a three-year contract with the studio and made his acting debut. Chow became a heartthrob and familiar face in soap operas that were exported internationally. According to Chow Yun-fat's filmography, Chow made his debut in 1976 in various movies produced by Goldig Films, including Hot Blood (ÂÖ•ÂÜä).  Goldig Films was founded by Gouw Hiap Kian and produced or distributed over 100 movies from 1972 to 1982. Chow's first movie contract was an exclusive acting contract with Goldig Films (note page 3). Chow appeared in the 1980 TV series The Bund on TVB. The series, about the rise and fall of a gangster in 1930s Shanghai, was a hit throughout Asia and made Chow a star. Although Chow continued his TV success, his goal was to become a film actor. However, his occasional ventures into low-budget films in the 1980s after ones by Goldig were disastrous. Most of Chow's movies produced by Goldig Films under exclusive contract in the 1970s achieved high gross revenues of over HK$ 1m per movie. These figures are higher than ones Chow acted in the early 1980s, including Modern Heroes (Ê±üÊπñÊ™îÊ°à), Soul Ash (ÁÅ∞Èùà), The Bund(‰∏äÊµ∑ÁÅò), The Bund Part 2(‰∏äÊµ∑ÁÅòÁ∫åÈõÜ) . Note gross revenues under list of movies. Success finally came when he teamed up with director John Woo in the 1986 gangster action-melodrama A Better Tomorrow, which swept the box offices in Asia and established Chow and Woo as megastars. A Better Tomorrow won him his first Best Actor award at the Hong Kong Film Awards. It was the highest-grossing film in Hong Kong history at the time, and set a new standard for Hong Kong gangster films. Taking the opportunity, Chow quit TV entirely. With his new image from A Better Tomorrow, he made many more 'gun fu' or 'heroic bloodshed' films, such as A Better Tomorrow 2 (1987), Prison on Fire (1987), Prison on Fire II (1991), The Killer (1989), A Better Tomorrow 3 (1990), Hard Boiled (1992) and City on Fire (1987), an inspiration for Quentin Tarantino's Reservoir Dogs. Chow may be best known for playing honorable tough guys, whether cops or criminals, but he has also starred in comedies like Diary of a Big Man (1988) and Now You See Love, Now You Don't (1992) and romantic blockbusters such as Love in a Fallen City (1984) and An Autumn's Tale (1987), for which he was named Best Actor at the Golden Horse Awards. He brought together his disparate personae in the 1989 film God of Gamblers, directed by the prolific Wong Jing, in which he was by turns a suave charmer, a broad comedian, and an action hero. The film surprised many, became immensely popular, broke Hong Kong's all-time box office record, and spawned a series of gambling films as well as several comic sequels starring Andy Lau and Stephen Chow. The often tough demeanour and youthful appearance of Chow's characters has earned him the nickname 'Babyface Killer'. Chow Yun-fat at the premiere of Pirates of the Caribbean: At World's End in 2007. The Los Angeles Times proclaimed Chow Yun-Fat 'the coolest actor in the world'. In the mid '90s, Chow moved to Hollywood in an ultimately unsuccessful attempt to duplicate his success in Asia. His first two films, The Replacement Killers (1998) and The Corruptor (1999), were box office failures. In his next film Anna and the King (1999), Chow teamed up with Jodie Foster, but the film underperformed at the box office. Chow accepted the role of Li Mu-Bai in the (2000) film Crouching Tiger, Hidden Dragon. It became a winner at both the international box office and the Oscars. In 2003, Chow came back to Hollywood and starred in Bulletproof Monk. In 2004, Chow made a surprise cameo in director Dayyan Eng's Chinese rom-com favourite Waiting Alone, it was the first time he was in a mainland Chinese film. In 2006, he teamed up with Gong Li in the film Curse of the Golden Flower, directed by Zhang Yimou. In 2007, Chow played the pirate captain Sao Feng in Pirates of the Caribbean: At World's End. However, his part was omitted when the movie was shown in mainland China, where government censors felt that Chow's character 'vilified and humiliated' Chinese people. In the poorly received film Dragonball Evolution, Chow Yun-fat played Master Roshi. In 2014, Chow returned to Hong Kong cinema in From Vegas to Macau. For the part, he lost 13 kg within 10 months. In 2015 and 2016, Chow reprised his role as Ken in the sequels From Vegas to Macau II and From Vegas to Macau III. In 2018, he co-starred with Aaron Kwok in Project Gutenberg which earned him another Best Actor nomination at the 38th Hong Kong Film Awards. On 26 June 2008, Chow released his first photo collection, which includes pictures taken on the sets of his films. Proceeds from the book's sales were donated to Sichuan earthquake victims. It is published by Louis Vuitton. Chow has been married twice; first was in 1983 to Candice Yu, an actress from Asia Television; the marriage lasted nine months. In 1986, Chow married Singaporean Jasmine Tan. They had a stillborn daughter in 1991. Chow has a goddaughter, Celine Ng, a former child model for Chickeeduck, McDonald's, Toys'R'Us and other companies. Despite his wealth, Chow lives modestly. He is frequently seen at food stalls and on public transportation. In interviews, he has said he plans to leave his fortune to charity. In October 2014, Chow voiced support for students in the Umbrella Movement, a civil rights movement for universal suffrage in Hong Kong. Chow has appeared in over 95 films and over 25 television series. Hong Kong Film Awards he won are : Best Actor Nomination for Hong Kong 1941. Best Actor Nomination for Women, Best Supporting Actor Nomination for Love Unto Waste, Best Actor for A Better Tomorrow, Best Actor Nomination for Prison on Fire, Best Actor Nomination for An Autumn's Tale, Best Actor for City on Fire, Best Original Film Song Nomination for The Diary of a Big Man, Best Original Film Song Nomination for Triads: The Inside Story, Best Actor Nomination for God of Gamblers, Best Actor for All About Ah-Long, Best Actor Nomination for Once a Thief, Best Actor Nomination for Treasure Hunt, Best Actor Nomination for Peace Hotel, Best Actor Nomination for Crouching Tiger, Hidden Dragon, Best Actor Nomination for Curse of the Golden Flower, Best Supporting Actor Nomination for The Postmodern Life of My Aunt, Best Actor Nomination for Project Gutenberg, (14 Best Actor nominations, two Best Supporting Actor nominations, two Best Original Film Song nominations). In Chinese American Film Festival he won Golden Angel for Best Actor in a Leading Role for Project Gutenberg (2019). In 2014, Chow was the second-highest earning actor in Hong Kong, earning HK$170 million (US$21.9 million). His reported net worth is HK$5.6 billion (US$714 million). In 2018, Chow's wife Jasmine Tan informed various Hong Kong media the figure HK$5.6b of Chow's net worth, which was not verified by any third party. Chow also said he would donate 99% of his wealth to charity via setting up a foundation to help those in need. There have been no other reports on who controls the foundation and its ultimate beneficiaries. His University Honoary Awards are Hong Kong Academy for Performing Arts - Honorary Fellow (1999) , City University of Hong Kong - Honorary Doctor of Letters (2001) , Hong Kong Baptist University - Doctor of Humanities, honoris causa (2021)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chow Yun-fat SBS (born 18 May 1955), previously known as Donald Chow, is a Hong Kong actor known for his collaborations with filmmaker John Woo in the action heroic bloodshed films A Better Tomorrow, The Killer, and Hard Boiled, and in the West for his roles as Li Mu-bai in Crouching Tiger, Hidden Dragon and Sao Feng in Pirates of the Caribbean: At World's End. He mainly plays in drama films and has won three Hong Kong Film Awards for Best Actor and two Golden Horse Awards for Best Actor in Taiwan. Chow started his career in movies in 1976 with Goldig Films. Chow was born in Lamma Island, Hong Kong, to Chow Yung-Wan (Âë®ÂÆπÂÖÅ), who worked on a Shell Oil Company tanker, and Chan Lai-fong (Èô≥È∫óËä≥), who was a cleaning lady and vegetable farmer. Chow grew up in a farming community on Lamma Island, in a house with no electricity. He woke up at dawn each morning to help his mother sell herbal jelly and Hakka tea-pudding (ÂÆ¢ÂÆ∂Ëå∂Á≤ø) on the streets; in the afternoons, he went to work in the fields. His family moved to Kowloon when he was ten. At 17, Chow left school to help support the family by doing odd jobs including a bellboy, postman, camera salesman, and taxi driver. Chow's life started to change after college when he responded to a newspaper advertisement, and his actor-trainee application was accepted by TVB, the local television station. He signed a three-year contract with the studio and made his acting debut. Chow became a heartthrob and familiar face in soap operas that were exported internationally. According to Chow Yun-fat's filmography, Chow made his debut in 1976 in various movies produced by Goldig Films, including Hot Blood (ÂÖ•ÂÜä).  Goldig Films was founded by Gouw Hiap Kian and produced or distributed over 100 movies from 1972 to 1982. Chow's first movie contract was an exclusive acting contract with Goldig Films (note page 3). Chow appeared in the 1980 TV series The Bund on TVB. The series, about the rise and fall of a gangster in 1930s Shanghai, was a hit throughout Asia and made Chow a star. Although Chow continued his TV success, his goal was to become a film actor. However, his occasional ventures into low-budget films in the 1980s after ones by Goldig were disastrous. Most of Chow's movies produced by Goldig Films under exclusive contract in the 1970s achieved high gross revenues of over HK$ 1m per movie. These figures are higher than ones Chow acted in the early 1980s, including Modern Heroes (Ê±üÊπñÊ™îÊ°à), Soul Ash (ÁÅ∞Èùà), The Bund(‰∏äÊµ∑ÁÅò), The Bund Part 2(‰∏äÊµ∑ÁÅòÁ∫åÈõÜ) . Note gross revenues under list of movies. Success finally came when he teamed up with director John Woo in the 1986 gangster action-melodrama A Better Tomorrow, which swept the box offices in Asia and established Chow and Woo as megastars. A Better Tomorrow won him his first Best Actor award at the Hong Kong Film Awards. It was the highest-grossing film in Hong Kong history at the time, and set a new standard for Hong Kong gangster films. Taking the opportunity, Chow quit TV entirely. With his new image from A Better Tomorrow, he made many more 'gun fu' or 'heroic bloodshed' films, such as A Better Tomorrow 2 (1987), Prison on Fire (1987), Prison on Fire II (1991), The Killer (1989), A Better Tomorrow 3 (1990), Hard Boiled (1992) and City on Fire (1987), an inspiration for Quentin Tarantino's Reservoir Dogs. Chow may be best known for playing honorable tough guys, whether cops or criminals, but he has also starred in comedies like Diary of a Big Man (1988) and Now You See Love, Now You Don't (1992) and romantic blockbusters such as Love in a Fallen City (1984) and An Autumn's Tale (1987), for which he was named Best Actor at the Golden Horse Awards. He brought together his disparate personae in the 1989 film God of Gamblers, directed by the prolific Wong Jing, in which he was by turns a suave charmer, a broad comedian, and an action hero. The film surprised many, became immensely popular, broke Hong Kong's all-time box office record, and spawned a series of gambling films as well as several comic sequels starring Andy Lau and Stephen Chow. The often tough demeanour and youthful appearance of Chow's characters has earned him the nickname 'Babyface Killer'. Chow Yun-fat at the premiere of Pirates of the Caribbean: At World's End in 2007. The Los Angeles Times proclaimed Chow Yun-Fat 'the coolest actor in the world'. In the mid '90s, Chow moved to Hollywood in an ultimately unsuccessful attempt to duplicate his success in Asia. His first two films, The Replacement Killers (1998) and The Corruptor (1999), were box office failures. In his next film Anna and the King (1999), Chow teamed up with Jodie Foster, but the film underperformed at the box office. Chow accepted the role of Li Mu-Bai in the (2000) film Crouching Tiger, Hidden Dragon. It became a winner at both the international box office and the Oscars. In 2003, Chow came back to Hollywood and starred in Bulletproof Monk. In 2004, Chow made a surprise cameo in director Dayyan Eng's Chinese rom-com favourite Waiting Alone, it was the first time he was in a mainland Chinese film. In 2006, he teamed up with Gong Li in the film Curse of the Golden Flower, directed by Zhang Yimou. In 2007, Chow played the pirate captain Sao Feng in Pirates of the Caribbean: At World's End. However, his part was omitted when the movie was shown in mainland China, where government censors felt that Chow's character 'vilified and humiliated' Chinese people. In the poorly received film Dragonball Evolution, Chow Yun-fat played Master Roshi. In 2014, Chow returned to Hong Kong cinema in From Vegas to Macau. For the part, he lost 13 kg within 10 months. In 2015 and 2016, Chow reprised his role as Ken in the sequels From Vegas to Macau II and From Vegas to Macau III. In 2018, he co-starred with Aaron Kwok in Project Gutenberg which earned him another Best Actor nomination at the 38th Hong Kong Film Awards. On 26 June 2008, Chow released his first photo collection, which includes pictures taken on the sets of his films. Proceeds from the book's sales were donated to Sichuan earthquake victims. It is published by Louis Vuitton. Chow has been married twice; first was in 1983 to Candice Yu, an actress from Asia Television; the marriage lasted nine months. In 1986, Chow married Singaporean Jasmine Tan. They had a stillborn daughter in 1991. Chow has a goddaughter, Celine Ng, a former child model for Chickeeduck, McDonald's, Toys'R'Us and other companies. Despite his wealth, Chow lives modestly. He is frequently seen at food stalls and on public transportation. In interviews, he has said he plans to leave his fortune to charity. In October 2014, Chow voiced support for students in the Umbrella Movement, a civil rights movement for universal suffrage in Hong Kong. Chow has appeared in over 95 films and over 25 television series. Hong Kong Film Awards he won are : Best Actor Nomination for Hong Kong 1941. Best Actor Nomination for Women, Best Supporting Actor Nomination for Love Unto Waste, Best Actor for A Better Tomorrow, Best Actor Nomination for Prison on Fire, Best Actor Nomination for An Autumn's Tale, Best Actor for City on Fire, Best Original Film Song Nomination for The Diary of a Big Man, Best Original Film Song Nomination for Triads: The Inside Story, Best Actor Nomination for God of Gamblers, Best Actor for All About Ah-Long, Best Actor Nomination for Once a Thief, Best Actor Nomination for Treasure Hunt, Best Actor Nomination for Peace Hotel, Best Actor Nomination for Crouching Tiger, Hidden Dragon, Best Actor Nomination for Curse of the Golden Flower, Best Supporting Actor Nomination for The Postmodern Life of My Aunt, Best Actor Nomination for Project Gutenberg, (14 Best Actor nominations, two Best Supporting Actor nominations, two Best Original Film Song nominations). In Chinese American Film Festival he won Golden Angel for Best Actor in a Leading Role for Project Gutenberg (2019). In 2014, Chow was the second-highest earning actor in Hong Kong, earning HK$170 million (US$21.9 million). His reported net worth is HK$5.6 billion (US$714 million). In 2018, Chow's wife Jasmine Tan informed various Hong Kong media the figure HK$5.6b of Chow's net worth, which was not verified by any third party. Chow also said he would donate 99% of his wealth to charity via setting up a foundation to help those in need. There have been no other reports on who controls the foundation and its ultimate beneficiaries. His University Honoary Awards are Hong Kong Academy for Performing Arts - Honorary Fellow (1999) , City University of Hong Kong - Honorary Doctor of Letters (2001) , Hong Kong Baptist University - Doctor of Humanities, honoris causa (2021)\n"
     ]
    }
   ],
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions on Chow Yun Fatt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Who is Chow Yun-fat's wife?\n",
    "# Q2. When was Chow Yun-fat born?\n",
    "# Q3. Which show did he acted in 2007 and what was his role? ** \n",
    "# Q4. What are his nominations?\n",
    "# Q5. What are Chow Yun-fat's University Honoary Awards?\n",
    "# Q6. What did Chow Yun-fat do in 2003?\n",
    "# Q7. What is his nickname?\n",
    "# Q8. How many giraffes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1966) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_z/rz8y91hx7lzcvrljph2rnzlm0000gp/T/ipykernel_9683/3335661053.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPlease enter your question: \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mquestion_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_z/rz8y91hx7lzcvrljph2rnzlm0000gp/T/ipykernel_9683/1677506268.py\u001b[0m in \u001b[0;36mquestion_answer\u001b[0;34m(question, text)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#model output using input_ids and segment_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#reconstructing the answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/longformer/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/longformer/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1586\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m         )\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/longformer/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/longformer/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         embedding_output = self.embeddings(\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         )\n\u001b[1;32m    827\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m~/miniconda3/envs/longformer/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/longformer/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1966) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# 2 options to input text. Either 1. assign to a variable text or 2. input into the chatbot at start\n",
    "# text = input(\"Please enter your text: \\n\")\n",
    "question = input(\"\\nPlease enter your question: \\n\")\n",
    "while True:\n",
    "    question_answer(question, text)\n",
    "    \n",
    "    flag = True\n",
    "    flag_N = False\n",
    "    \n",
    "    while flag:\n",
    "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
    "        if response[0] == \"Y\":\n",
    "            question = input(\"\\nPlease enter your question: \\n\")\n",
    "            flag = False\n",
    "        elif response[0] == \"N\":\n",
    "            print(\"\\nBye!\")\n",
    "            flag = False\n",
    "            flag_N = True\n",
    "            \n",
    "    if flag_N == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626; http://www.conradweb.org/~jackg/pubs/ICAIL21_Vold_Conrad.pdf; "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "060d54d4ce858688ad6231c7ce8d6f336c144f153828bd33366cca7c89f987e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('chatbot': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
